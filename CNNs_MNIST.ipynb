{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNs MNIST",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYtwAav8vtqg"
      },
      "source": [
        "# Convolutional Neural Network\n",
        "\n",
        "## CNNs for hand-written digit (MNIST) classification\n",
        "\n",
        "**CNNs, by preserving the locality of the pixels, find much better features in the images to predict from than NNs.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mna10cRBwpBS"
      },
      "source": [
        "Start GPU. Should say \"Found GPU ...\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfTo4Ir6viY6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "333ac057-0fdd-4781-b0ac-c337ae8eedcc"
      },
      "source": [
        "import tensorflow as tf  \n",
        "                      \n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sjfthmqnw6Hn"
      },
      "source": [
        "import numpy as np  \n",
        "np.random.seed(123)  \n",
        "import tensorflow as tf  \n",
        "\n",
        "from keras.models import Sequential, load_model, clone_model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten \n",
        "from keras.layers import Conv2D, MaxPooling2D  \n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint \n",
        "from keras.datasets import mnist\n",
        "\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a82xulemvY5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6cec8a8-7a11-4bb7-e6f5-292dd7060e81"
      },
      "source": [
        "# Load pre-shuffled MNIST data into train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(X_train.shape) \n",
        "print(y_train.shape) \n",
        "print(X_test.shape)  \n",
        "print(y_test.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3slRt7HCoi3x"
      },
      "source": [
        "### Data pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga1cNehewIpH"
      },
      "source": [
        "# reshape for single grey channel\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1) \n",
        "input_shape = (28, 28, 1) \n",
        "\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_Ho_qT_wKu3"
      },
      "source": [
        "# normalise from 0 to 1\n",
        "X_train = X_train.astype('float32')\n",
        "X_train /= 255 \n",
        "\n",
        "X_test = X_test.astype('float32')\n",
        "X_test /= 255"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJhy0NIOwZdh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73d3702e-d99b-48d2-b89a-9d7988af3aae"
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "print(y_train.shape)\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "print(Y_train.shape)\n",
        "\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000,)\n",
            "(60000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APQvXnI8wvDo"
      },
      "source": [
        "### Define the CNN model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WaRmiJawoXq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c70d86e7-7f38-4157-b831-7e6e20b40df3"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1))) # 32 filters of size 3x3, then ReLu the output\n",
        "print(model.output_shape)\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation='relu')) \n",
        "print(model.output_shape)\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2))) # Down-sample in boxes of size 2x2 with no-overlap. Allows for some translation invariance in the image.\n",
        "print(model.output_shape)\n",
        "\n",
        "model.add(Flatten()) # Convert to one long vector that represents the image in terms of the learned features\n",
        "print(model.output_shape)\n",
        "\n",
        "model.add(Dense(128, activation='relu')) # The thinking (fully-connected) layer. Learn the interactions between the extracted features and the desired output\n",
        "print(model.output_shape)\n",
        "\n",
        "model.add(Dense(10, activation='softmax')) # The dense layer with softmax activation to output 10 probabilities - one for each digit\n",
        "print(model.output_shape)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(model.summary()) "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 26, 26, 32)\n",
            "(None, 24, 24, 32)\n",
            "(None, 12, 12, 32)\n",
            "(None, 4608)\n",
            "(None, 128)\n",
            "(None, 10)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 24, 24, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               589952    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 600,810\n",
            "Trainable params: 600,810\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnlk6pHhy7QQ"
      },
      "source": [
        "### Train the CNN model\n",
        "10 epochs and a batch size of 32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzlY03BvzDfZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "529703f4-aac3-4cd8-94f0-83813943d2dc"
      },
      "source": [
        "checkpointer = ModelCheckpoint('CNN-{epoch:02d}.hdf5', verbose = 1) # Temporarily save the model to our drive after each epoch, with the epoch number \n",
        "model.fit(X_train, Y_train, batch_size=32, epochs=10, verbose=1, callbacks=[checkpointer]) # Verbose = 1 shows the progress of the model training"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 11s 3ms/step - loss: 0.2596 - accuracy: 0.9219\n",
            "\n",
            "Epoch 00001: saving model to CNN-01.hdf5\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0389 - accuracy: 0.9883\n",
            "\n",
            "Epoch 00002: saving model to CNN-02.hdf5\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0230 - accuracy: 0.9931\n",
            "\n",
            "Epoch 00003: saving model to CNN-03.hdf5\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0155 - accuracy: 0.9954\n",
            "\n",
            "Epoch 00004: saving model to CNN-04.hdf5\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0117 - accuracy: 0.9958\n",
            "\n",
            "Epoch 00005: saving model to CNN-05.hdf5\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0077 - accuracy: 0.9974\n",
            "\n",
            "Epoch 00006: saving model to CNN-06.hdf5\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0060 - accuracy: 0.9980\n",
            "\n",
            "Epoch 00007: saving model to CNN-07.hdf5\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0046 - accuracy: 0.9985\n",
            "\n",
            "Epoch 00008: saving model to CNN-08.hdf5\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0052 - accuracy: 0.9983\n",
            "\n",
            "Epoch 00009: saving model to CNN-09.hdf5\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0036 - accuracy: 0.9987\n",
            "\n",
            "Epoch 00010: saving model to CNN-10.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3a109e69e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITBa5XUdzJC3"
      },
      "source": [
        "### Test the CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJH3wUitzHfu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1c9d329-50b8-4d93-e64e-b5399a94d981"
      },
      "source": [
        "model_score = model.evaluate(X_test, Y_test, verbose = 1)\n",
        "print('Test loss: ', model_score[0])\n",
        "print('Test accuracy: ', model_score[1])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0494 - accuracy: 0.9905\n",
            "Test loss:  0.04937336966395378\n",
            "Test accuracy:  0.9904999732971191\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIMAGFFIrIcU"
      },
      "source": [
        "CNN model: 99% accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w82G7ZYNrWZD"
      },
      "source": [
        "### Redefine model (model2). More filters and dropout layers added. Optimiser changed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29JOdW8jzjn0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d0ed011-cd85-430a-e7d2-c61af53d53eb"
      },
      "source": [
        "model2 = Sequential() \n",
        "\n",
        "model2.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=(28, 28, 1)))\n",
        "\n",
        "model2.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model2.add(Dropout(0.25)) # reduce overfitting in the model, should help performance\n",
        "\n",
        "model2.add(Flatten())\n",
        "\n",
        "model2.add(Dense(128, activation='relu'))\n",
        "\n",
        "model2.add(Dropout(0.5))\n",
        "\n",
        "model2.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model2.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adadelta', # instead of 'adam'\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(model2.summary())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 9216)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               1179776   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,199,882\n",
            "Trainable params: 1,199,882\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV6aiDdj0smK"
      },
      "source": [
        "### Train CNN model2\n",
        "*12 epochs and a batch size of 128*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1M5w-Kd0oG7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d991a900-9c0d-4e54-bd85-ace4d833638b"
      },
      "source": [
        "model2.fit(X_train, Y_train, batch_size=128, epochs=12, verbose=1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "469/469 [==============================] - 3s 5ms/step - loss: 2.2883 - accuracy: 0.1338\n",
            "Epoch 2/12\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 2.2429 - accuracy: 0.2555\n",
            "Epoch 3/12\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 2.1794 - accuracy: 0.3717\n",
            "Epoch 4/12\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 2.0939 - accuracy: 0.4470\n",
            "Epoch 5/12\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.9812 - accuracy: 0.5051\n",
            "Epoch 6/12\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.8431 - accuracy: 0.5437\n",
            "Epoch 7/12\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.6803 - accuracy: 0.5826\n",
            "Epoch 8/12\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.5177 - accuracy: 0.6119\n",
            "Epoch 9/12\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.3641 - accuracy: 0.6399\n",
            "Epoch 10/12\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.2255 - accuracy: 0.6687\n",
            "Epoch 11/12\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.1253 - accuracy: 0.6814\n",
            "Epoch 12/12\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.0400 - accuracy: 0.7006\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f399eca4518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKk1B0La1C2A"
      },
      "source": [
        "### Test CNN model2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYmJgXxe1CDq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87a388ed-56f8-496e-f949-a274ec00a346"
      },
      "source": [
        "model2_score = model2.evaluate(X_test, Y_test, verbose = 1)\n",
        "print('Test loss: ', model2_score[0])\n",
        "print('Test accuracy: ', model2_score[1])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.7770 - accuracy: 0.8305\n",
            "Test loss:  0.7769657969474792\n",
            "Test accuracy:  0.8305000066757202\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk6h6BSe1SXb"
      },
      "source": [
        "CNN model2: 83% accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbLZ2Z7eC38o"
      },
      "source": [
        "# Classify an uploaded real image\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSh5Vq3lEp6T"
      },
      "source": [
        "# quick function to load an image, turn it to greyscale and resize it to 28x28\n",
        "\n",
        "from PIL import Image\n",
        "import PIL.ImageOps  # turn the image into a black number\n",
        "\n",
        "def file_to_input(string):\n",
        "  img  = Image.open(string) \n",
        "  width, height = img.size \n",
        "  area = (width/4, height/4, 3*width/4, 3*height/4) \n",
        "  img = img.crop(area) # crops to the middle of the image\n",
        "\n",
        "  img = img.resize((28, 28)) # resize the image to 28x28\n",
        "  img = img.convert('L') # turn to greyscale\n",
        "  img = PIL.ImageOps.invert(img)\n",
        "\n",
        "  pic = np.array(img)\n",
        "  pic = pic/255\n",
        "  # np.place(pic, pic < 0.5, 0) # these could possibly used to sharpen the images...\n",
        "  # np.place(pic, pic > 0.5, 1)\n",
        "\n",
        "  return pic"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIEzK9wpKadA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "40636248-6199-4ef8-8c3b-27dd4fee687e"
      },
      "source": [
        "img1 = file_to_input('seven.jpg')\n",
        "print(img1.shape)\n",
        "plt.imshow(img1, cmap='Greys')\n",
        "img1_reshape = img1.reshape(1,28,28,1)\n",
        "result = model2.predict(img1_reshape)\n",
        "print(np.argmax(result))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28, 28)\n",
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skcpF_N2NhCa"
      },
      "source": [
        "# Close out all the images we opened\n",
        "#plt.close('all')\n",
        "#Image.close('all')"
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}